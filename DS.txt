<-----DS1----->
import pandas as pd
d_url = "DS1_Bengaluru_House_Data.csv"
df = pd.read_csv(d_url)
df.head()

print("\nVariable Descriptions:")
print(df.info())

print("\nChecking Missing Values:")
print(df.isnull().sum())

print("\nChecking Statistical Values:")
print(df.describe())

print("\nDimensions of Data Frame:")
print(df.shape)

df.fillna('NA', inplace=True)

print("\nChecking Missing Values:")
print(df.isnull().sum())

print("\nData Types of Variables:")
print(df.dtypes)

df['area_type'] = df['area_type'].astype('category').cat.codes
df['location'] = df['location'].astype('category').cat.codes
df['size'] = df['size'].str.replace('Bedroom','BHK')

print("\nCleaned Data:")
df

<-----DS2----->
import pandas as pd
import numpy as np

data = {'ID': [1, 2, 3, 4, 5],
        'Name': ['Shreyash', 'Manthan', 'Tanmay', 'Shweta', 'Tejas'],
        'Gender': ['Male', 'Male', 'Male', 'Female', 'Male'],
        'Age': [25, 21, 23, 26, 22],
        'Class': ['Fresher', 'Fresher', 'Junior', 'Junior', 'Senior'],
        'Major': ['Science', 'Engineering', 'Arts', 'Business', 'Science'],
        'GPA': [3.5, 3.5, 3.8, 2.9, 3.6],
        'Test Score': [85, 90, 93, 75, 87],
        'Attendance': [90, 85, 80, 95, 75]}
df = pd.DataFrame(data)

print(df.isnull().sum())

print(df['Gender'].value_counts())

print(df[['Age','GPA', 'Test Score', 'Attendance']].describe())

import matplotlib.pyplot as plt
df[['Age','GPA', 'Test Score', 'Attendance']].boxplot()
plt.show

plt.scatter(df['GPA'], df['Test Score'])
plt.xlabel('GPA')
plt.show()

df['Log Test Score'] = np.log(df['Test Score'])
plt.hist(df['Log Test Score'])
plt.xlabel('Log Test Score')
plt.ylabel('Frequency')
plt.show()

import scipy.stats as stats
stats.probplot(df['Log Test Score'], plot=plt)
plt.show()

<-----DS3----->
import pandas as pd

df = pd.read_csv('Iris.csv')
df.head()

df.isnull().sum()

df.isna().sum()

df.describe()

df.groupby('SepalLengthCm')['SepalWidthCm'].describe()

df.groupby('SepalLengthCm')['SepalWidthCm'].mean()

df[df['Species']=='Iris-setosa'].describe()

df[df['Species']=='Iris-versicolor'].describe()

df[df['Species']=='Iris-virginica'].describe()

<-----DS4----->
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
import seaborn as sns

df = pd.read_csv('submission_example.csv')
df.head()

df.info()

df.drop('ID', axis=1, inplace=True)
df

test_df = pd.read_csv('test.csv')
test_df.head()

df1 = pd.DataFrame()
df1["rm"] = test_df.rm
df1["medv"] = df.medv
df1

df1.plot.scatter("rm","medv")

x = test_df[['crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','black','lstat']]
y = df['medv']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4)

model = LinearRegression()
model.fit(x_train,y_train)
predictions = model.predict(x_test)

plt.scatter(y_test,predictions)
plt.xlabel('Y test')
plt.ylabel('Predicted Y')
plt.show()

print('MAE:', metrics.mean_absolute_error(y_test, predictions))
print('MSE:', metrics.mean_squared_error(y_test, predictions))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))

sns.histplot((y_test-predictions),bins=50)

<-----DS5----->
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

df = pd.read_csv('Social_Network_Ads.csv')
df.head()

x = df.iloc[:, [2, 3]].values
y = df.iloc[:, 4].values
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)

SS = StandardScaler()
x_train = SS.fit_transform(x_train)
x_test = SS.transform(x_test)

model = LogisticRegression(random_state=0)
model.fit(x_train,y_train)
y_pred = model.predict(x_test)

CM = confusion_matrix(y_test,y_pred)
print("Confusion Matrix",CM)
TP = CM[1, 1]
FP = CM[0, 1]
TN = CM[0, 0]
FN = CM[1, 0]
print("True Positives (TP):", TP)
print("False Positives (FP):", FP)
print("True Negatives (TN):", TN)
print("False Negatives (FN):", FN)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
error_rate = 1 - accuracy
print("Error Rate:", error_rate)
precision = precision_score(y_test, y_pred)
print("Precision:", precision)
recall = recall_score(y_test, y_pred)
print("Recall:", recall)

<-----DS6----->
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score

df = pd.read_csv('Iris.csv')
df.head()

x = df.drop('Species', axis=1)
y = df['Species']
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3)

NBC = GaussianNB()
NBC.fit(x_train,y_train)
y_pred = NBC.predict(x_test)

CM = confusion_matrix(y_test,y_pred)
TP = CM[1,1]
FP = CM[0,1]
TN = CM[0,0]
FN = CM[1,0]
print("True Positives (TP):", TP)
print("False Positives (FP):", FP)
print("True Negatives (TN):", TN)
print("False Negatives (FN):", FN)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
error_rate = 1 - accuracy
print("Error Rate:", error_rate)
precision = precision_score(y_test, y_pred, average='macro')
print("Precision:", precision)
recall = recall_score(y_test, y_pred, average='macro')
print("Recall:", recall)

<-----DS7----->
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from collections import Counter

# import nltk #------> word tokenization
# nltk.download('phunkt')
# import nltk #----> pos tag
# nltk.download('averaged_perceptron_tagger')
# from nltk.corpus import stopwords #------->stopwords
# nltk.download('stopwords')
# import nltk #----> lemmatization
# nltk.download('wordnet')

data = "Sachin is considered to be one of the greatest cricket player. MS Dhoni is the captain of the Indian cricket team"
tokens = word_tokenize(data)
print("Tokens:", tokens)

pos_tags = nltk.pos_tag(tokens)
print("POS Tags:", pos_tags)

stop_words = set(stopwords.words('english'))
print(stop_words)
filtered_tokens = [token for token in tokens if token.lower() not in stop_words]
print("Filtered Tokens:", filtered_tokens)

stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]
print("Stemmed Tokens:", stemmed_tokens)

lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
print("Lemmatized Tokens:", lemmatized_tokens)

tf = Counter(lemmatized_tokens)
print("Term Frequency:", tf)

I_datas = ["Sachin is considered to be one of the greatest cricket player.",
"Ronaldo and Messi are considered to be greatest football players among all.",
"Michael Jordan id considered to be one of the greatest basketball players.",
" MS Dhoni is the captain of the Indian cricket team."]

import math
def idf(term, I_datas):
    count = sum(1 for doc in I_datas if term in doc)
    if count == 0:
        return 0
    else:
        return math.log(len(I_datas) / count)

idf_values = {term: idf(term, I_datas) for term in set(lemmatized_tokens)}
print("Inverse Document Frequency:", idf_values)

<-----DS8----->
import seaborn as sns
import matplotlib.pyplot as plt

titanic_data = sns.load_dataset('titanic')
print(titanic_data.to_string())

sns.pairplot(titanic_data)
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(x='class', data=titanic_data)
plt.title('Passenger Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(6, 4))
sns.histplot(titanic_data['fare'], bins=20)
plt.title('Distribution of Ticket Prices')
plt.xlabel('Ticket Fare')
plt.ylabel('Frequency')
plt.show()

<-----DS9----->
import seaborn as sns
import matplotlib.pyplot as plt

titanic_data = sns.load_dataset('titanic')
titanic_data.head()

# import numpy as np
sns.boxplot(x='sex', y='age', hue='survived', data=titanic_data)
# plt.yticks(np.arange(0,85,5))
# plt.legend(title='Survived',labels=["No","Yes"])
plt.show()

<-----DS10----->
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data = sns.load_dataset('iris')
data.head()

data.info()

nf = data.drop('species', axis=1)
nf.hist(figsize=(10, 8))
plt.show()

plt.figure(figsize=(10, 8))
sns.boxplot(data=nf)
plt.show()